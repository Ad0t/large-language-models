{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c188841d",
   "metadata": {},
   "source": [
    "# Building a Byte Pair Encoding (BPE) Tokenizer from Scratch\n",
    "## Step 1: Prepare Training Data (just creating list of tokens from data)\n",
    "The first step in building any tokenizer is to have a corpus of text to train it on. The tokenizer learns merge rules based on the frequency of character pairs in this data.  \n",
    "- i: 1\n",
    "- s: 2\n",
    "- is: 3  \n",
    "\n",
    "Even though \"i\" and \"s\" are separate tokens, we create a new token \"is\" by merging them as they frequently appear together (is, this, his, miss, dismiss, list, fist, twist, mist, whisk, visible, vision, revise, crisis), reducing computation needs by 2x at any place where we merge those 2 tokens. This is how we will itteratively merge most frequent pairs. The new tokens can also be further merged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77509c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Corpus:\n",
      "This is the first document.\n",
      "This document is the second document.\n",
      "And this is the third one.\n",
      "Is this the first document?\n"
     ]
    }
   ],
   "source": [
    "# Our sample training data\n",
    "corpus = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\",\n",
    "]\n",
    "print(\"Training Corpus:\")\n",
    "for doc in corpus:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec76621",
   "metadata": {},
   "source": [
    "## Step 2: Initialize Vocabulary and Pre-tokenize\n",
    "The BPE algorithm starts with a base vocabulary consisting of all unique characters present in the training data.  \n",
    "\n",
    "We also need to pre-tokenize the corpus. This usually involves splitting the text into words (or word-like units) and then representing each word as a sequence of its individual characters. We often add a special end-of-word token (like `</w>`) to mark word boundaries, which helps the tokenizer learn subword units that align better with whole words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c688ce9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Vocabulary:\n",
      "[' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>']\n",
      "Vocabulary Size: 20\n",
      "\n",
      "Pre-tokenized Word Frequencies:\n",
      "{('T', 'h', 'i', 's', '</w>'): 2, ('i', 's', '</w>'): 3, ('t', 'h', 'e', '</w>'): 4, ('f', 'i', 'r', 's', 't', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '.', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('t', 'h', 'i', 's', '</w>'): 2, ('t', 'h', 'i', 'r', 'd', '</w>'): 1, ('o', 'n', 'e', '.', '</w>'): 1, ('I', 's', '</w>'): 1, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '?', '</w>'): 1}\n"
     ]
    }
   ],
   "source": [
    "# Initialize vocabulary with unique characters\n",
    "unique_chars = set()\n",
    "for doc in corpus:\n",
    "    for char in doc:\n",
    "        unique_chars.add(char)\n",
    "\n",
    "vocab = list(unique_chars)\n",
    "vocab.sort() # For consistent order of characters, making the vocabulary list predictable\n",
    "# print(f\"Initial Vocabulary:\\n{vocab}\")\n",
    "# Add a special end-of-word token\n",
    "end_of_word = \"</w>\"\n",
    "vocab.append(end_of_word)\n",
    "\n",
    "print(f\"Initial Vocabulary:\\n{vocab}\\nVocabulary Size: {len(vocab)}\")\n",
    "\n",
    "# Pre-tokenize the corpus: Split into words and then characters\n",
    "# We'll split by space for simplicity and add the end-of-word token\n",
    "word_splits = {}\n",
    "for doc in corpus:\n",
    "    words = doc.split(\" \")\n",
    "    for word in words:\n",
    "        if word:\n",
    "            character_list = list(word) + [end_of_word]\n",
    "            # Use tuple for immutability if storing counts later - you can't change tuple once it's created (values, order, adding, removing elements, etc.), so they can be used as dictionary keys because of that.\n",
    "            word_tuple = tuple(character_list)\n",
    "            if word_tuple not in word_splits:\n",
    "                word_splits[word_tuple] = 0\n",
    "            word_splits[word_tuple] += 1 # Count frequency of each initial word split\n",
    "\n",
    "print(f\"\\nPre-tokenized Word Frequencies:\\n{word_splits}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15089c23",
   "metadata": {},
   "source": [
    "## Helper Function: `get_pair_stats`\n",
    "This function takes the current word splits (represented as a dictionary where keys are tuples of symbols/characters forming a word and values are their frequencies) and calculates the frequency of each adjacent pair of symbols across the entire corpus.\n",
    "\n",
    "**Input Example (`splits`):**\n",
    "```\n",
    "{('T', 'h', 'i', 's', '</w>'): 2, ('i', 's', '</w>'): 2, ...}\n",
    "```\n",
    "**Output Example (`pair_counts`):**\n",
    "```\n",
    "{('i', 's'): 4, ('s', '</w>'): 4, ('T', 'h'): 2, ...}\n",
    "```\n",
    "Initialize a dictionary with default values of 0 to count pairs of symbols.  \n",
    "defaultdict: It's like a regular dictionary (dict), but with a key difference.  \n",
    "If you try to access or modify a key that doesn't exist, instead of raising a KeyError, it automatically creates that key and assigns it a default value.  \n",
    "int: This is the \"default factory\" you provide when creating the defaultdict. When a new key is created, it needs a default value, defaultdict calls this factory function. int() called with no arguments returns 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce250067",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "def get_pair_stats(splits):\n",
    "    \"\"\"Counts the frequency of adjacent pairs in the word splits.\"\"\"\n",
    "    pair_counts = collections.defaultdict(int)\n",
    "    for word_tuple, freq in splits.items():\n",
    "        symbols = list(word_tuple)\n",
    "        for i in range(len(symbols) - 1):\n",
    "            pair = (symbols[i], symbols[i+1])\n",
    "            pair_counts[pair] += freq # Add the frequency of the word to the pair count\n",
    "    return pair_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d5fbb5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('T', 'h'): 2, ('h', 'i'): 5, ('i', 's'): 7, ('s', '</w>'): 8, ('t', 'h'): 7, ('h', 'e'): 4, ('e', '</w>'): 4, ('f', 'i'): 2, ('i', 'r'): 3, ('r', 's'): 2, ('s', 't'): 2, ('t', '</w>'): 3, ('d', 'o'): 4, ('o', 'c'): 4, ('c', 'u'): 4, ('u', 'm'): 4, ('m', 'e'): 4, ('e', 'n'): 4, ('n', 't'): 4, ('t', '.'): 2, ('.', '</w>'): 3, ('s', 'e'): 1, ('e', 'c'): 1, ('c', 'o'): 1, ('o', 'n'): 2, ('n', 'd'): 2, ('d', '</w>'): 3, ('A', 'n'): 1, ('r', 'd'): 1, ('n', 'e'): 1, ('e', '.'): 1, ('I', 's'): 1, ('t', '?'): 1, ('?', '</w>'): 1}\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "x = get_pair_stats(word_splits)\n",
    "print(dict(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d357c77",
   "metadata": {},
   "source": [
    "## Helper Function: `merge_pair`\n",
    "This function takes a specific pair (`pair_to_merge`) that we want to combine and the current `splits`. It iterates through all the word representations in `splits`, replaces occurrences of the `pair_to_merge` with a new single token (concatenation of the pair), and returns the updated `splits`.\n",
    "**Input Example:**\n",
    "```\n",
    "`pair_to_merge`: `('i', 's')`\n",
    "`splits`: `{('T', 'h', 'i', 's', '</w>'): 2, ('i', 's', '</w>'): 2, ...}`\n",
    "```\n",
    "**Output Example (`new_splits`):**\n",
    "```\n",
    "`{('T', 'h', 'is', '</w>'): 2, ('is', '</w>'): 2, ...}` (assuming 'is' is the merged token)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d30bf9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_pair(pair_to_merge, splits):\n",
    "    \"\"\"Merges the specified pair in the word splits.\"\"\"\n",
    "    new_splits = {}\n",
    "    (first, second) = pair_to_merge\n",
    "    merged_token = first + second\n",
    "    for word_tuple, freq in splits.items():\n",
    "        symbols = list(word_tuple)\n",
    "        new_symbols = []\n",
    "        i = 0\n",
    "        while i < len(symbols):\n",
    "            # If the current and next symbol match the pair to merge\n",
    "            if i < len(symbols) - 1 and symbols[i] == first and symbols[i+1] == second:\n",
    "                new_symbols.append(merged_token)\n",
    "                i += 2 # Skips the next\n",
    "            else:\n",
    "                new_symbols.append(symbols[i])\n",
    "                i += 1\n",
    "        new_splits[tuple(new_symbols)] = freq # Use the updated symbol list as the key\n",
    "    return new_splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e5bdb5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('T', 'h', 'is', '</w>'): 2, ('is', '</w>'): 3, ('t', 'h', 'e', '</w>'): 4, ('f', 'i', 'r', 's', 't', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '.', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('t', 'h', 'is', '</w>'): 2, ('t', 'h', 'i', 'r', 'd', '</w>'): 1, ('o', 'n', 'e', '.', '</w>'): 1, ('I', 's', '</w>'): 1, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '?', '</w>'): 1}\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "y = merge_pair('is', word_splits)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a7dc1d",
   "metadata": {},
   "source": [
    "## Step 3: Iterative BPE Merging Loop\n",
    "Now we perform the core BPE training. We'll loop for a fixed number of merges (`num_merges`). In each iteration:  \n",
    "1. Calculate the frequencies of all adjacent pairs in the current word representations using `get_pair_stats`.\n",
    "2. Find the pair with the highest frequency (`best_pair`).\n",
    "3. Merge this `best_pair` across all word representations using `merge_pair`.\n",
    "4. Add the newly formed token (concatenation of `best_pair`) to our vocabulary (`vocab`).\n",
    "5. Store the merge rule (mapping the pair to the new token) in the `merges` dictionary.  \n",
    "We'll add print statements to observe the state at each step of the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "15bdfd59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting BPE Merges ---\n",
      "Initial Splits: {('T', 'h', 'i', 's', '</w>'): 2, ('i', 's', '</w>'): 3, ('t', 'h', 'e', '</w>'): 4, ('f', 'i', 'r', 's', 't', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '.', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('t', 'h', 'i', 's', '</w>'): 2, ('t', 'h', 'i', 'r', 'd', '</w>'): 1, ('o', 'n', 'e', '.', '</w>'): 1, ('I', 's', '</w>'): 1, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '?', '</w>'): 1}\n",
      "------------------------------\n",
      "\n",
      "Merge Iteration 1/15\n",
      "Top 5 Pair Frequencies: [(('s', '</w>'), 8), (('i', 's'), 7), (('t', 'h'), 7), (('h', 'i'), 5), (('h', 'e'), 4)]\n",
      "Found Best Pair: ('s', '</w>') with Frequency: 8\n",
      "Merging ('s', '</w>') into 's</w>'\n",
      "Splits after merge: {('T', 'h', 'i', 's</w>'): 2, ('i', 's</w>'): 3, ('t', 'h', 'e', '</w>'): 4, ('f', 'i', 'r', 's', 't', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '.', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('t', 'h', 'i', 's</w>'): 2, ('t', 'h', 'i', 'r', 'd', '</w>'): 1, ('o', 'n', 'e', '.', '</w>'): 1, ('I', 's</w>'): 1, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '?', '</w>'): 1}\n",
      "Updated Vocabulary: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>', 's</w>', 'is</w>', 'th', 'the', 'the</w>', 'do', 'doc', 'docu', 'docum', 'docume', 'documen', 'document', 'ir', '.</w>', 'd</w>', 's</w>']\n",
      "Updated Merges: {('s', '</w>'): 's</w>'}\n",
      "------------------------------\n",
      "\n",
      "Merge Iteration 2/15\n",
      "Top 5 Pair Frequencies: [(('i', 's</w>'), 7), (('t', 'h'), 7), (('h', 'i'), 5), (('h', 'e'), 4), (('e', '</w>'), 4)]\n",
      "Found Best Pair: ('i', 's</w>') with Frequency: 7\n",
      "Merging ('i', 's</w>') into 'is</w>'\n",
      "Splits after merge: {('T', 'h', 'is</w>'): 2, ('is</w>',): 3, ('t', 'h', 'e', '</w>'): 4, ('f', 'i', 'r', 's', 't', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '.', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('t', 'h', 'is</w>'): 2, ('t', 'h', 'i', 'r', 'd', '</w>'): 1, ('o', 'n', 'e', '.', '</w>'): 1, ('I', 's</w>'): 1, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '?', '</w>'): 1}\n",
      "Updated Vocabulary: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>', 's</w>', 'is</w>', 'th', 'the', 'the</w>', 'do', 'doc', 'docu', 'docum', 'docume', 'documen', 'document', 'ir', '.</w>', 'd</w>', 's</w>', 'is</w>']\n",
      "Updated Merges: {('s', '</w>'): 's</w>', ('i', 's</w>'): 'is</w>'}\n",
      "------------------------------\n",
      "\n",
      "Merge Iteration 3/15\n",
      "Top 5 Pair Frequencies: [(('t', 'h'), 7), (('h', 'is</w>'), 4), (('h', 'e'), 4), (('e', '</w>'), 4), (('d', 'o'), 4)]\n",
      "Found Best Pair: ('t', 'h') with Frequency: 7\n",
      "Merging ('t', 'h') into 'th'\n",
      "Splits after merge: {('T', 'h', 'is</w>'): 2, ('is</w>',): 3, ('th', 'e', '</w>'): 4, ('f', 'i', 'r', 's', 't', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '.', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('th', 'is</w>'): 2, ('th', 'i', 'r', 'd', '</w>'): 1, ('o', 'n', 'e', '.', '</w>'): 1, ('I', 's</w>'): 1, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '?', '</w>'): 1}\n",
      "Updated Vocabulary: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>', 's</w>', 'is</w>', 'th', 'the', 'the</w>', 'do', 'doc', 'docu', 'docum', 'docume', 'documen', 'document', 'ir', '.</w>', 'd</w>', 's</w>', 'is</w>', 'th']\n",
      "Updated Merges: {('s', '</w>'): 's</w>', ('i', 's</w>'): 'is</w>', ('t', 'h'): 'th'}\n",
      "------------------------------\n",
      "\n",
      "Merge Iteration 4/15\n",
      "Top 5 Pair Frequencies: [(('th', 'e'), 4), (('e', '</w>'), 4), (('d', 'o'), 4), (('o', 'c'), 4), (('c', 'u'), 4)]\n",
      "Found Best Pair: ('th', 'e') with Frequency: 4\n",
      "Merging ('th', 'e') into 'the'\n",
      "Splits after merge: {('T', 'h', 'is</w>'): 2, ('is</w>',): 3, ('the', '</w>'): 4, ('f', 'i', 'r', 's', 't', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '.', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('th', 'is</w>'): 2, ('th', 'i', 'r', 'd', '</w>'): 1, ('o', 'n', 'e', '.', '</w>'): 1, ('I', 's</w>'): 1, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '?', '</w>'): 1}\n",
      "Updated Vocabulary: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>', 's</w>', 'is</w>', 'th', 'the', 'the</w>', 'do', 'doc', 'docu', 'docum', 'docume', 'documen', 'document', 'ir', '.</w>', 'd</w>', 's</w>', 'is</w>', 'th', 'the']\n",
      "Updated Merges: {('s', '</w>'): 's</w>', ('i', 's</w>'): 'is</w>', ('t', 'h'): 'th', ('th', 'e'): 'the'}\n",
      "------------------------------\n",
      "\n",
      "Merge Iteration 5/15\n",
      "Top 5 Pair Frequencies: [(('the', '</w>'), 4), (('d', 'o'), 4), (('o', 'c'), 4), (('c', 'u'), 4), (('u', 'm'), 4)]\n",
      "Found Best Pair: ('the', '</w>') with Frequency: 4\n",
      "Merging ('the', '</w>') into 'the</w>'\n",
      "Splits after merge: {('T', 'h', 'is</w>'): 2, ('is</w>',): 3, ('the</w>',): 4, ('f', 'i', 'r', 's', 't', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '.', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('th', 'is</w>'): 2, ('th', 'i', 'r', 'd', '</w>'): 1, ('o', 'n', 'e', '.', '</w>'): 1, ('I', 's</w>'): 1, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '?', '</w>'): 1}\n",
      "Updated Vocabulary: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>', 's</w>', 'is</w>', 'th', 'the', 'the</w>', 'do', 'doc', 'docu', 'docum', 'docume', 'documen', 'document', 'ir', '.</w>', 'd</w>', 's</w>', 'is</w>', 'th', 'the', 'the</w>']\n",
      "Updated Merges: {('s', '</w>'): 's</w>', ('i', 's</w>'): 'is</w>', ('t', 'h'): 'th', ('th', 'e'): 'the', ('the', '</w>'): 'the</w>'}\n",
      "------------------------------\n",
      "\n",
      "Merge Iteration 6/15\n",
      "Top 5 Pair Frequencies: [(('d', 'o'), 4), (('o', 'c'), 4), (('c', 'u'), 4), (('u', 'm'), 4), (('m', 'e'), 4)]\n",
      "Found Best Pair: ('d', 'o') with Frequency: 4\n",
      "Merging ('d', 'o') into 'do'\n",
      "Splits after merge: {('T', 'h', 'is</w>'): 2, ('is</w>',): 3, ('the</w>',): 4, ('f', 'i', 'r', 's', 't', '</w>'): 2, ('do', 'c', 'u', 'm', 'e', 'n', 't', '.', '</w>'): 2, ('do', 'c', 'u', 'm', 'e', 'n', 't', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('th', 'is</w>'): 2, ('th', 'i', 'r', 'd', '</w>'): 1, ('o', 'n', 'e', '.', '</w>'): 1, ('I', 's</w>'): 1, ('do', 'c', 'u', 'm', 'e', 'n', 't', '?', '</w>'): 1}\n",
      "Updated Vocabulary: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>', 's</w>', 'is</w>', 'th', 'the', 'the</w>', 'do', 'doc', 'docu', 'docum', 'docume', 'documen', 'document', 'ir', '.</w>', 'd</w>', 's</w>', 'is</w>', 'th', 'the', 'the</w>', 'do']\n",
      "Updated Merges: {('s', '</w>'): 's</w>', ('i', 's</w>'): 'is</w>', ('t', 'h'): 'th', ('th', 'e'): 'the', ('the', '</w>'): 'the</w>', ('d', 'o'): 'do'}\n",
      "------------------------------\n",
      "\n",
      "Merge Iteration 7/15\n",
      "Top 5 Pair Frequencies: [(('do', 'c'), 4), (('c', 'u'), 4), (('u', 'm'), 4), (('m', 'e'), 4), (('e', 'n'), 4)]\n",
      "Found Best Pair: ('do', 'c') with Frequency: 4\n",
      "Merging ('do', 'c') into 'doc'\n",
      "Splits after merge: {('T', 'h', 'is</w>'): 2, ('is</w>',): 3, ('the</w>',): 4, ('f', 'i', 'r', 's', 't', '</w>'): 2, ('doc', 'u', 'm', 'e', 'n', 't', '.', '</w>'): 2, ('doc', 'u', 'm', 'e', 'n', 't', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('th', 'is</w>'): 2, ('th', 'i', 'r', 'd', '</w>'): 1, ('o', 'n', 'e', '.', '</w>'): 1, ('I', 's</w>'): 1, ('doc', 'u', 'm', 'e', 'n', 't', '?', '</w>'): 1}\n",
      "Updated Vocabulary: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>', 's</w>', 'is</w>', 'th', 'the', 'the</w>', 'do', 'doc', 'docu', 'docum', 'docume', 'documen', 'document', 'ir', '.</w>', 'd</w>', 's</w>', 'is</w>', 'th', 'the', 'the</w>', 'do', 'doc']\n",
      "Updated Merges: {('s', '</w>'): 's</w>', ('i', 's</w>'): 'is</w>', ('t', 'h'): 'th', ('th', 'e'): 'the', ('the', '</w>'): 'the</w>', ('d', 'o'): 'do', ('do', 'c'): 'doc'}\n",
      "------------------------------\n",
      "\n",
      "Merge Iteration 8/15\n",
      "Top 5 Pair Frequencies: [(('doc', 'u'), 4), (('u', 'm'), 4), (('m', 'e'), 4), (('e', 'n'), 4), (('n', 't'), 4)]\n",
      "Found Best Pair: ('doc', 'u') with Frequency: 4\n",
      "Merging ('doc', 'u') into 'docu'\n",
      "Splits after merge: {('T', 'h', 'is</w>'): 2, ('is</w>',): 3, ('the</w>',): 4, ('f', 'i', 'r', 's', 't', '</w>'): 2, ('docu', 'm', 'e', 'n', 't', '.', '</w>'): 2, ('docu', 'm', 'e', 'n', 't', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('th', 'is</w>'): 2, ('th', 'i', 'r', 'd', '</w>'): 1, ('o', 'n', 'e', '.', '</w>'): 1, ('I', 's</w>'): 1, ('docu', 'm', 'e', 'n', 't', '?', '</w>'): 1}\n",
      "Updated Vocabulary: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>', 's</w>', 'is</w>', 'th', 'the', 'the</w>', 'do', 'doc', 'docu', 'docum', 'docume', 'documen', 'document', 'ir', '.</w>', 'd</w>', 's</w>', 'is</w>', 'th', 'the', 'the</w>', 'do', 'doc', 'docu']\n",
      "Updated Merges: {('s', '</w>'): 's</w>', ('i', 's</w>'): 'is</w>', ('t', 'h'): 'th', ('th', 'e'): 'the', ('the', '</w>'): 'the</w>', ('d', 'o'): 'do', ('do', 'c'): 'doc', ('doc', 'u'): 'docu'}\n",
      "------------------------------\n",
      "\n",
      "Merge Iteration 9/15\n",
      "Top 5 Pair Frequencies: [(('docu', 'm'), 4), (('m', 'e'), 4), (('e', 'n'), 4), (('n', 't'), 4), (('i', 'r'), 3)]\n",
      "Found Best Pair: ('docu', 'm') with Frequency: 4\n",
      "Merging ('docu', 'm') into 'docum'\n",
      "Splits after merge: {('T', 'h', 'is</w>'): 2, ('is</w>',): 3, ('the</w>',): 4, ('f', 'i', 'r', 's', 't', '</w>'): 2, ('docum', 'e', 'n', 't', '.', '</w>'): 2, ('docum', 'e', 'n', 't', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('th', 'is</w>'): 2, ('th', 'i', 'r', 'd', '</w>'): 1, ('o', 'n', 'e', '.', '</w>'): 1, ('I', 's</w>'): 1, ('docum', 'e', 'n', 't', '?', '</w>'): 1}\n",
      "Updated Vocabulary: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>', 's</w>', 'is</w>', 'th', 'the', 'the</w>', 'do', 'doc', 'docu', 'docum', 'docume', 'documen', 'document', 'ir', '.</w>', 'd</w>', 's</w>', 'is</w>', 'th', 'the', 'the</w>', 'do', 'doc', 'docu', 'docum']\n",
      "Updated Merges: {('s', '</w>'): 's</w>', ('i', 's</w>'): 'is</w>', ('t', 'h'): 'th', ('th', 'e'): 'the', ('the', '</w>'): 'the</w>', ('d', 'o'): 'do', ('do', 'c'): 'doc', ('doc', 'u'): 'docu', ('docu', 'm'): 'docum'}\n",
      "------------------------------\n",
      "\n",
      "Merge Iteration 10/15\n",
      "Top 5 Pair Frequencies: [(('docum', 'e'), 4), (('e', 'n'), 4), (('n', 't'), 4), (('i', 'r'), 3), (('t', '</w>'), 3)]\n",
      "Found Best Pair: ('docum', 'e') with Frequency: 4\n",
      "Merging ('docum', 'e') into 'docume'\n",
      "Splits after merge: {('T', 'h', 'is</w>'): 2, ('is</w>',): 3, ('the</w>',): 4, ('f', 'i', 'r', 's', 't', '</w>'): 2, ('docume', 'n', 't', '.', '</w>'): 2, ('docume', 'n', 't', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('th', 'is</w>'): 2, ('th', 'i', 'r', 'd', '</w>'): 1, ('o', 'n', 'e', '.', '</w>'): 1, ('I', 's</w>'): 1, ('docume', 'n', 't', '?', '</w>'): 1}\n",
      "Updated Vocabulary: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>', 's</w>', 'is</w>', 'th', 'the', 'the</w>', 'do', 'doc', 'docu', 'docum', 'docume', 'documen', 'document', 'ir', '.</w>', 'd</w>', 's</w>', 'is</w>', 'th', 'the', 'the</w>', 'do', 'doc', 'docu', 'docum', 'docume']\n",
      "Updated Merges: {('s', '</w>'): 's</w>', ('i', 's</w>'): 'is</w>', ('t', 'h'): 'th', ('th', 'e'): 'the', ('the', '</w>'): 'the</w>', ('d', 'o'): 'do', ('do', 'c'): 'doc', ('doc', 'u'): 'docu', ('docu', 'm'): 'docum', ('docum', 'e'): 'docume'}\n",
      "------------------------------\n",
      "\n",
      "Merge Iteration 11/15\n",
      "Top 5 Pair Frequencies: [(('docume', 'n'), 4), (('n', 't'), 4), (('i', 'r'), 3), (('t', '</w>'), 3), (('.', '</w>'), 3)]\n",
      "Found Best Pair: ('docume', 'n') with Frequency: 4\n",
      "Merging ('docume', 'n') into 'documen'\n",
      "Splits after merge: {('T', 'h', 'is</w>'): 2, ('is</w>',): 3, ('the</w>',): 4, ('f', 'i', 'r', 's', 't', '</w>'): 2, ('documen', 't', '.', '</w>'): 2, ('documen', 't', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('th', 'is</w>'): 2, ('th', 'i', 'r', 'd', '</w>'): 1, ('o', 'n', 'e', '.', '</w>'): 1, ('I', 's</w>'): 1, ('documen', 't', '?', '</w>'): 1}\n",
      "Updated Vocabulary: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>', 's</w>', 'is</w>', 'th', 'the', 'the</w>', 'do', 'doc', 'docu', 'docum', 'docume', 'documen', 'document', 'ir', '.</w>', 'd</w>', 's</w>', 'is</w>', 'th', 'the', 'the</w>', 'do', 'doc', 'docu', 'docum', 'docume', 'documen']\n",
      "Updated Merges: {('s', '</w>'): 's</w>', ('i', 's</w>'): 'is</w>', ('t', 'h'): 'th', ('th', 'e'): 'the', ('the', '</w>'): 'the</w>', ('d', 'o'): 'do', ('do', 'c'): 'doc', ('doc', 'u'): 'docu', ('docu', 'm'): 'docum', ('docum', 'e'): 'docume', ('docume', 'n'): 'documen'}\n",
      "------------------------------\n",
      "\n",
      "Merge Iteration 12/15\n",
      "Top 5 Pair Frequencies: [(('documen', 't'), 4), (('i', 'r'), 3), (('t', '</w>'), 3), (('.', '</w>'), 3), (('d', '</w>'), 3)]\n",
      "Found Best Pair: ('documen', 't') with Frequency: 4\n",
      "Merging ('documen', 't') into 'document'\n",
      "Splits after merge: {('T', 'h', 'is</w>'): 2, ('is</w>',): 3, ('the</w>',): 4, ('f', 'i', 'r', 's', 't', '</w>'): 2, ('document', '.', '</w>'): 2, ('document', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('th', 'is</w>'): 2, ('th', 'i', 'r', 'd', '</w>'): 1, ('o', 'n', 'e', '.', '</w>'): 1, ('I', 's</w>'): 1, ('document', '?', '</w>'): 1}\n",
      "Updated Vocabulary: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>', 's</w>', 'is</w>', 'th', 'the', 'the</w>', 'do', 'doc', 'docu', 'docum', 'docume', 'documen', 'document', 'ir', '.</w>', 'd</w>', 's</w>', 'is</w>', 'th', 'the', 'the</w>', 'do', 'doc', 'docu', 'docum', 'docume', 'documen', 'document']\n",
      "Updated Merges: {('s', '</w>'): 's</w>', ('i', 's</w>'): 'is</w>', ('t', 'h'): 'th', ('th', 'e'): 'the', ('the', '</w>'): 'the</w>', ('d', 'o'): 'do', ('do', 'c'): 'doc', ('doc', 'u'): 'docu', ('docu', 'm'): 'docum', ('docum', 'e'): 'docume', ('docume', 'n'): 'documen', ('documen', 't'): 'document'}\n",
      "------------------------------\n",
      "\n",
      "Merge Iteration 13/15\n",
      "Top 5 Pair Frequencies: [(('i', 'r'), 3), (('.', '</w>'), 3), (('d', '</w>'), 3), (('T', 'h'), 2), (('h', 'is</w>'), 2)]\n",
      "Found Best Pair: ('i', 'r') with Frequency: 3\n",
      "Merging ('i', 'r') into 'ir'\n",
      "Splits after merge: {('T', 'h', 'is</w>'): 2, ('is</w>',): 3, ('the</w>',): 4, ('f', 'ir', 's', 't', '</w>'): 2, ('document', '.', '</w>'): 2, ('document', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('th', 'is</w>'): 2, ('th', 'ir', 'd', '</w>'): 1, ('o', 'n', 'e', '.', '</w>'): 1, ('I', 's</w>'): 1, ('document', '?', '</w>'): 1}\n",
      "Updated Vocabulary: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>', 's</w>', 'is</w>', 'th', 'the', 'the</w>', 'do', 'doc', 'docu', 'docum', 'docume', 'documen', 'document', 'ir', '.</w>', 'd</w>', 's</w>', 'is</w>', 'th', 'the', 'the</w>', 'do', 'doc', 'docu', 'docum', 'docume', 'documen', 'document', 'ir']\n",
      "Updated Merges: {('s', '</w>'): 's</w>', ('i', 's</w>'): 'is</w>', ('t', 'h'): 'th', ('th', 'e'): 'the', ('the', '</w>'): 'the</w>', ('d', 'o'): 'do', ('do', 'c'): 'doc', ('doc', 'u'): 'docu', ('docu', 'm'): 'docum', ('docum', 'e'): 'docume', ('docume', 'n'): 'documen', ('documen', 't'): 'document', ('i', 'r'): 'ir'}\n",
      "------------------------------\n",
      "\n",
      "Merge Iteration 14/15\n",
      "Top 5 Pair Frequencies: [(('.', '</w>'), 3), (('d', '</w>'), 3), (('T', 'h'), 2), (('h', 'is</w>'), 2), (('f', 'ir'), 2)]\n",
      "Found Best Pair: ('.', '</w>') with Frequency: 3\n",
      "Merging ('.', '</w>') into '.</w>'\n",
      "Splits after merge: {('T', 'h', 'is</w>'): 2, ('is</w>',): 3, ('the</w>',): 4, ('f', 'ir', 's', 't', '</w>'): 2, ('document', '.</w>'): 2, ('document', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('th', 'is</w>'): 2, ('th', 'ir', 'd', '</w>'): 1, ('o', 'n', 'e', '.</w>'): 1, ('I', 's</w>'): 1, ('document', '?', '</w>'): 1}\n",
      "Updated Vocabulary: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>', 's</w>', 'is</w>', 'th', 'the', 'the</w>', 'do', 'doc', 'docu', 'docum', 'docume', 'documen', 'document', 'ir', '.</w>', 'd</w>', 's</w>', 'is</w>', 'th', 'the', 'the</w>', 'do', 'doc', 'docu', 'docum', 'docume', 'documen', 'document', 'ir', '.</w>']\n",
      "Updated Merges: {('s', '</w>'): 's</w>', ('i', 's</w>'): 'is</w>', ('t', 'h'): 'th', ('th', 'e'): 'the', ('the', '</w>'): 'the</w>', ('d', 'o'): 'do', ('do', 'c'): 'doc', ('doc', 'u'): 'docu', ('docu', 'm'): 'docum', ('docum', 'e'): 'docume', ('docume', 'n'): 'documen', ('documen', 't'): 'document', ('i', 'r'): 'ir', ('.', '</w>'): '.</w>'}\n",
      "------------------------------\n",
      "\n",
      "Merge Iteration 15/15\n",
      "Top 5 Pair Frequencies: [(('d', '</w>'), 3), (('T', 'h'), 2), (('h', 'is</w>'), 2), (('f', 'ir'), 2), (('ir', 's'), 2)]\n",
      "Found Best Pair: ('d', '</w>') with Frequency: 3\n",
      "Merging ('d', '</w>') into 'd</w>'\n",
      "Splits after merge: {('T', 'h', 'is</w>'): 2, ('is</w>',): 3, ('the</w>',): 4, ('f', 'ir', 's', 't', '</w>'): 2, ('document', '.</w>'): 2, ('document', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd</w>'): 1, ('A', 'n', 'd</w>'): 1, ('th', 'is</w>'): 2, ('th', 'ir', 'd</w>'): 1, ('o', 'n', 'e', '.</w>'): 1, ('I', 's</w>'): 1, ('document', '?', '</w>'): 1}\n",
      "Updated Vocabulary: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>', 's</w>', 'is</w>', 'th', 'the', 'the</w>', 'do', 'doc', 'docu', 'docum', 'docume', 'documen', 'document', 'ir', '.</w>', 'd</w>', 's</w>', 'is</w>', 'th', 'the', 'the</w>', 'do', 'doc', 'docu', 'docum', 'docume', 'documen', 'document', 'ir', '.</w>', 'd</w>']\n",
      "Updated Merges: {('s', '</w>'): 's</w>', ('i', 's</w>'): 'is</w>', ('t', 'h'): 'th', ('th', 'e'): 'the', ('the', '</w>'): 'the</w>', ('d', 'o'): 'do', ('do', 'c'): 'doc', ('doc', 'u'): 'docu', ('docu', 'm'): 'docum', ('docum', 'e'): 'docume', ('docume', 'n'): 'documen', ('documen', 't'): 'document', ('i', 'r'): 'ir', ('.', '</w>'): '.</w>', ('d', '</w>'): 'd</w>'}\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- BPE Training Loop Initialization ---\n",
    "num_merges = 15\n",
    "# Stores merge rules, e.g., {('a', 'b'): 'ab'}\n",
    "# Example: {('T', 'h'): 'Th'}\n",
    "merges = {}\n",
    "# Initial word splits: {('T', 'h', 'i', 's', '</w>'): 2, ('i', 's', '</w>'): 2, ...}\n",
    "current_splits = word_splits.copy() # Start with initial word splits\n",
    "\n",
    "print(f\"\\n--- Starting BPE Merges ---\\nInitial Splits: {current_splits}\\n{\"-\" * 30}\")\n",
    "\n",
    "for i in range(num_merges):\n",
    "    print(f\"\\nMerge Iteration {i+1}/{num_merges}\")\n",
    "\n",
    "    # 1. Calculate Pair Frequencies\n",
    "    pair_stats = get_pair_stats(current_splits)\n",
    "    if not pair_stats:\n",
    "        print(\"No more pairs to merge.\")\n",
    "        break\n",
    "    # Optional: Print top 5 pairs for inspection\n",
    "    sorted_pairs = sorted(pair_stats.items(), key=lambda item: item[1], reverse=True)\n",
    "    print(f\"Top 5 Pair Frequencies: {sorted_pairs[:5]}\")\n",
    "\n",
    "    # 2. Find Best Pair\n",
    "    # The 'max' function iterates over all key-value pairs in the 'pair_stats' dictionary\n",
    "    # The 'key=pair_stats.get' tells 'max' to use the frequency (value) for comparison, not the pair (key) itself\n",
    "    # This way, 'max' selects the pair with the highest frequency\n",
    "    best_pair = max(pair_stats, key=pair_stats.get)\n",
    "    best_freq = pair_stats[best_pair]\n",
    "    print(f\"Found Best Pair: {best_pair} with Frequency: {best_freq}\")\n",
    "\n",
    "    # 3. Merge the Best Pair\n",
    "    current_splits = merge_pair(best_pair, current_splits)\n",
    "    new_token = best_pair[0] + best_pair[1]\n",
    "    print(f\"Merging {best_pair} into '{new_token}'\")\n",
    "    print(f\"Splits after merge: {current_splits}\")\n",
    "\n",
    "    # 4. Update Vocabulary\n",
    "    vocab.append(new_token)\n",
    "    print(f\"Updated Vocabulary: {vocab}\")\n",
    "\n",
    "    # 5. Store Merge Rule\n",
    "    merges[best_pair] = new_token\n",
    "    print(f\"Updated Merges: {merges}\")\n",
    "\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862a42e5",
   "metadata": {},
   "source": [
    "## Step 4: Review Final Results\n",
    "After the loop finishes, we can examine the final state:\n",
    "- The learned merge rules (`merges`).\n",
    "- The final representation of words after merges (`current_splits`).\n",
    "- The complete vocabulary (`vocab`) containing initial characters and learned subword tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f6ef6161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- BPE Merges Complete ---\n",
      "Final Vocabulary Size: 50\n",
      "Learned Merges (Pair -> New Token):\n",
      "('s', '</w>') -> 's</w>'\n",
      "('i', 's</w>') -> 'is</w>'\n",
      "('t', 'h') -> 'th'\n",
      "('th', 'e') -> 'the'\n",
      "('the', '</w>') -> 'the</w>'\n",
      "('d', 'o') -> 'do'\n",
      "('do', 'c') -> 'doc'\n",
      "('doc', 'u') -> 'docu'\n",
      "('docu', 'm') -> 'docum'\n",
      "('docum', 'e') -> 'docume'\n",
      "('docume', 'n') -> 'documen'\n",
      "('documen', 't') -> 'document'\n",
      "('i', 'r') -> 'ir'\n",
      "('.', '</w>') -> '.</w>'\n",
      "('d', '</w>') -> 'd</w>'\n",
      "\n",
      "Final Word Splits after all merges:\n",
      "{('T', 'h', 'is</w>'): 2, ('is</w>',): 3, ('the</w>',): 4, ('f', 'ir', 's', 't', '</w>'): 2, ('document', '.</w>'): 2, ('document', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd</w>'): 1, ('A', 'n', 'd</w>'): 1, ('th', 'is</w>'): 2, ('th', 'ir', 'd</w>'): 1, ('o', 'n', 'e', '.</w>'): 1, ('I', 's</w>'): 1, ('document', '?', '</w>'): 1}\n",
      "\n",
      "Final Vocabulary (sorted):\n",
      "[' ', '.', '.</w>', '</w>', '?', 'A', 'I', 'T', 'c', 'd', 'd</w>', 'do', 'doc', 'docu', 'docum', 'docume', 'documen', 'document', 'e', 'f', 'h', 'i', 'ir', 'is</w>', 'm', 'n', 'o', 'r', 's', 's</w>', 't', 'th', 'the', 'the</w>', 'u']\n"
     ]
    }
   ],
   "source": [
    "# --- BPE Merges Complete ---\n",
    "print(f\"\\n--- BPE Merges Complete ---\\nFinal Vocabulary Size: {len(vocab)}\\nLearned Merges (Pair -> New Token):\")\n",
    "# Pretty print merges\n",
    "for pair, token in merges.items():\n",
    "    print(f\"{pair} -> '{token}'\")\n",
    "\n",
    "print(\"\\nFinal Word Splits after all merges:\")\n",
    "print(current_splits)\n",
    "\n",
    "print(\"\\nFinal Vocabulary (sorted):\")\n",
    "# Sort for consistent viewing\n",
    "final_vocab_sorted = sorted(list(set(vocab))) # Use set to remove potential duplicates if any step introduced them\n",
    "print(final_vocab_sorted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
